# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ekomey/SVR-Regression/blob/Training/Regression.ipynb
"""

import io
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import warnings
warnings.filterwarnings("ignore")
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
import tensorflow.compat.v1 as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold
from math import sqrt
import sklearn.metrics as sm

from google.colab import files
uploaded = files.upload()

data = pd.read_csv("Metro_Interstate_Traffic_Volume.csv")
data.head()

le =LabelEncoder()

Holiday_labels = le.fit_transform(data['holiday'])
{index: label for index , label in enumerate(le.classes_)}

weather_main = le.fit_transform(data['weather_main'])
{index: label for index , label in enumerate(le.classes_)}

weather_description = le.fit_transform(data['weather_description'])
{index: label for index , label in enumerate(le.classes_)}

data['holiday'] = le.fit_transform(data['holiday'])
data['weather_main'] = le.fit_transform(data['weather_main'])
data['weather_description'] = le.fit_transform(data['weather_description'])

#Aggregate rows with same date-time
data_agg = data.groupby('date_time',as_index=False).agg('max')

#Convert date_time column to datetime format
data_agg['date_time'] = pd.to_datetime(data_agg['date_time'])

#Create column with hours from date_time
data_agg['hour'] = data_agg['date_time'].dt.hour
data_agg['hour'].value_counts().plot(kind='bar')

#Create column with day of the week from date_time
data_agg['day_of_week'] = data_agg['date_time'].dt.dayofweek
data_agg['day_of_week'].value_counts().plot(kind='bar')

#Create column with month from date_time
data_agg['month'] = data_agg['date_time'].dt.month
data_agg['month'].value_counts().plot(kind='bar')

data_agg.head()
data_agg.drop(['date_time'], axis=1)

#Dave preprocessed data to csv
data_agg.to_csv("Metro-Interstate-Traffic-Volume-Encoded.csv", index=False)

# We have imported all dependencied
df = pd.read_csv('Metro-Interstate-Traffic-Volume-Encoded.csv')
df = df.drop(['date_time'],axis=1) # Drop Date feature

print(df.info()) # Overview of dataset
df = df.dropna(inplace=False)  # Remove all nan entries.

# For normalizing dataset
def normalization(raw_data):
  for col_num in range(raw_data.shape[1]):
    if raw_data.iloc[:,col_num].dtype == np.float or raw_data.iloc[:,col_num].dtype == np.int:
      raw_data.iloc[:,col_num] = (raw_data.iloc[:,col_num] - raw_data.iloc[:,col_num].mean()) / (raw_data.iloc[:,col_num].max() - raw_data.iloc[:,col_num].min())
  return raw_data

# For denormalizing dataset
def denormalize(df,norm_data):
  df = df['traffic_volume'].values.reshape(-1,1)
  norm_data = norm_data.reshape(-1,1)
  scl = MinMaxScaler()
  a = scl.fit_transform(df)
  new = scl.inverse_transform(norm_data)
  return new

df = normalization(df)
x_data = df.drop('traffic_volume', axis=1).values
y_data = df['traffic_volume'][:,None]

df_train = df[:8115]    
df_test = df[8115:]

#Network parameters
n_hidden1 = 8
n_hidden2 = 8
n_hidden3 = 8
n_input = 10
n_output = 1

#Learning parameters
learning_constant = 0.01
number_epochs = 100
batch_size = 1000
n_split=10


def neural_network(input_d, n_input):
  #layer 1 multiplying and adding bias then activation function
  w1 = tf.Variable(tf.random_normal([n_input, n_hidden1]))
  b1 = tf.Variable(tf.random_normal([n_hidden1]))
  layer_1 = tf.nn.relu(tf.add(tf.matmul(input_d,w1), b1))

  #layer 2 multiplying and adding bias then activation function
  w2 = tf.Variable(tf.random_normal([n_hidden1, n_hidden2]))
  b2 = tf.Variable(tf.random_normal([n_hidden2]))
  layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1,w2), b2))

  #layer 3 multiplying and adding bias then activation function
  w3 = tf.Variable(tf.random_normal([n_hidden2, n_hidden3]))
  b3 = tf.Variable(tf.random_normal([n_hidden3]))
  layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2,w3), b3))

  #Output layer multiplying and adding bias then activation function (output layer has one node only)
  w4 = tf.Variable(tf.random_normal([n_hidden3, n_output]))
  b4 = tf.Variable(tf.random_normal([n_output]))
  output = tf.add(tf.matmul(layer_3,w4), b4)
   
  return output

#Defining the input and the output
tf.disable_v2_behavior()
X = tf.placeholder("float")
Y = tf.placeholder("float")

output = neural_network(X, n_input)

#Mean squared error cost function
loss_op = tf.reduce_mean(tf.math.squared_difference(output,Y))

#Gradinent Descent optimiztion for updating weights and biases
optimizer = tf.train.AdamOptimizer(learning_constant).minimize(loss_op)

c_t = []
c_test = []

with tf.Session() as sess:
  #Initiate session and initialize all vaiables
  sess.run(tf.global_variables_initializer())
  saver = tf.train.Saver()
  sum_rmse = 0

  for train_index,test_index in KFold(n_split).split(x_data):
    X_train,X_test=x_data[train_index],x_data[test_index]
    Y_train,Y_test=y_data[train_index],y_data[test_index]

    for i in range(number_epochs):
      total_batch = int(X_train.shape[0] / batch_size)
      for j in range(total_batch):
        batch_x = X_train[j*batch_size:(j+1)*batch_size]
        batch_y = Y_train[j*batch_size:(j+1)*batch_size]
        sess.run([loss_op, optimizer],feed_dict= {X:batch_x, Y:batch_y})
            
      c_t.append(sess.run(loss_op, feed_dict={X:X_train,Y:Y_train}))
      c_test.append(sess.run(loss_op, feed_dict={X:X_test,Y:Y_test}))
      if i % 10 == 0:
        print("------------------------------")
        print('Epoch :',i,'Cost :',c_t[i])

    #Predict output of test data after training
    pred = sess.run(output, feed_dict={X:X_test})
    print("==============================")
    print('[K Fold cross Validation]')
    print('Cost :',sess.run(loss_op, feed_dict={X:X_test,Y:Y_test}))

    #Calculate the Root Mean Squared Error
    mse = mean_squared_error(Y_test, pred)
    rmse = sqrt(mse)
    print("Root Mean Squared Error :", rmse)
    sum_rmse += rmse

  #Denormalize output data     
  Y_test = denormalize(df_test,Y_test)
  pred = denormalize(df_test,pred)

  #Calculate Average the Root Mean Squared Error
  avg_rmse = sum_rmse/n_split
  print("============================================")
  print("Average Root Mean Squared Error:", avg_rmse)
  print("============================================")

  print("Mean absolute error =", round(sm.mean_absolute_error(Y_test, pred), 2)) 
  print("Mean squared error =", round(sm.mean_squared_error(Y_test, pred), 2)) 
  print("Median absolute error =", round(sm.median_absolute_error(Y_test, pred), 2)) 
  print("Explain variance score =", round(sm.explained_variance_score(Y_test, pred), 2)) 
  print("R2 score =", round(sm.r2_score(Y_test, pred), 2))

  if input('Save model? [Y/N]') == 'Y':
    saver.save(sess,'Metro-Interstate-Traffic-Volume-Encoded_dataset.h5')
    print('Model Saved')

#Network parameters
n_hidden1 = 15
n_hidden2 = 10
n_hidden3 = 8
n_input = 13
n_output = 1
#Learning parameters
learning_constant = 0.001
number_epochs = 100
batch_size = 1000

#Network parameters
n_hidden1 = 10
n_hidden2 = 10
n_input = 2
n_output = 2
#Learning parameters
learning_constant = 0.2
number_epochs = 200
batch_size = 1000

#Plotting the graph 
plt.figure(figsize=(15,8))
plt.scatter (Y_test, pred) 
range = [Y_test.min(), pred.max()] 
plt.plot (range, range, 'red') 
plt.title('Predicted vs Actual ')
plt.xlabel ('Actual')
plt.ylabel ('Predicted ') 
plt.show ()

plt.figure(figsize=(15,8))
plt.plot(Y_test[:500],label="Original Data")
plt.plot(pred[:500],label="Predicted Data")
plt.legend(loc='best')
plt.ylabel('Traffic Volume')
plt.xlabel('Y_test')
plt.title('100 datapoints of Traffic Volume vs Y_test')
plt.show()