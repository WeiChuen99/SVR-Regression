{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekomey/SVR-Regression/blob/Training/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HIn9pMCTEkd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,RobustScaler\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "data = pd.read_csv(\"Metro_Interstate_Traffic_Volume.csv\")\n",
        "data.head()\n",
        "\n",
        "le =LabelEncoder()\n",
        "\n",
        "Holiday_labels = le.fit_transform(data['holiday'])\n",
        "\n",
        "{index: label for index , label in enumerate(le.classes_)}\n",
        "\n",
        "weather_main = le.fit_transform(data['weather_main'])\n",
        "{index: label for index , label in enumerate(le.classes_)}\n",
        "\n",
        "weather_description = le.fit_transform(data['weather_description'])\n",
        "{index: label for index , label in enumerate(le.classes_)}\n",
        "\n",
        "data['holiday'] = le.fit_transform(data['holiday'])\n",
        "data['weather_main'] = le.fit_transform(data['weather_main'])\n",
        "data['weather_description'] = le.fit_transform(data['weather_description'])\n",
        "\n",
        "\n",
        "# aggregate rows with same date-time\n",
        "data_agg = data.groupby('date_time',as_index=False).agg('max')\n",
        "\n",
        "data_agg['date_time'] = pd.to_datetime(data_agg['date_time'])\n",
        "# convert date_time column to datetime format\n",
        "\n",
        "# create column with hours from date_time\n",
        "data_agg['hour'] = data_agg['date_time'].dt.hour\n",
        "data_agg['hour'].value_counts().plot(kind='bar')\n",
        "\n",
        "# create column with day of the week from date_time\n",
        "data_agg['day_of_week'] = data_agg['date_time'].dt.dayofweek\n",
        "data_agg['day_of_week'].value_counts().plot(kind='bar')\n",
        "\n",
        "# create column with month from date_time\n",
        "data_agg['month'] = data_agg['date_time'].dt.month\n",
        "data_agg['month'].value_counts().plot(kind='bar')\n",
        "\n",
        "data_agg.head()\n",
        "data_agg.drop(['date_time'], axis=1)\n",
        "\n",
        "# save preprocessed data to csv\n",
        "data_agg.to_csv(\"Metro-Interstate-Traffic-Volume-Encoded.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0zs4iACE1xL"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow.compat.v1 as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# We have imported all dependencied\n",
        "import io\n",
        "df = pd.read_csv('Metro-Interstate-Traffic-Volume-Encoded.csv')\n",
        "#del data['date_time']\n",
        "df = df.drop(['date_time'],axis=1) # Drop Date feature\n",
        "print(df)\n",
        "\n",
        "print(df.info()) # Overview of dataset\n",
        "\n",
        "df = df.dropna(inplace=False)  # Remove all nan entries.\n",
        "\n",
        "df_train = df[:32460]    # 60% training data and 40% testing data\n",
        "df_test = df[32460:]\n",
        "# scaler = MinMaxScaler() # For normalizing dataset\n",
        "\n",
        "# # We want to predict Close value of stock \n",
        "# X_train = scaler.fit_transform(df_train.drop(['traffic_volume'],axis=1).values())\n",
        "# y_train = scaler.fit_transform(df_train['traffic_volume'].values())\n",
        "# # y is output and x is features.\n",
        "# X_test = scaler.fit_transform(df_test.drop(['traffic_volume'],axis=1).values())\n",
        "# y_test = scaler.fit_transform(df_test['traffic_volume'].values())\n",
        "\n",
        "def normalization(raw_data):\n",
        "    for col_num in range(raw_data.shape[1]):\n",
        "        if raw_data.iloc[:,col_num].dtype == np.float or raw_data.iloc[:,col_num].dtype == np.int:\n",
        "            raw_data.iloc[:,col_num] = (raw_data.iloc[:,col_num] - raw_data.iloc[:,col_num].mean()) / (raw_data.iloc[:,col_num].max() - raw_data.iloc[:,col_num].min())\n",
        "    return raw_data\n",
        "\n",
        "dataset = normalization(df)\n",
        "x_data = df.drop('traffic_volume',axis=1)\n",
        "y_data = df['traffic_volume'][:,None]\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(x_data,y_data,test_size=0.2,random_state=0)\n",
        "\n",
        "Y_train = np.reshape(Y_train, (-1, 1))\n",
        "y_test = np.reshape(Y_test, (-1, 1))\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "print(scaler_x.fit(X_train))\n",
        "xtrain_scale=scaler_x.transform(X_train)\n",
        "print(scaler_x.fit(X_test))\n",
        "xval_scale=scaler_x.transform(X_test)\n",
        "print(scaler_y.fit(Y_train))\n",
        "ytrain_scale=scaler_y.transform(Y_train)\n",
        "print(scaler_y.fit(Y_test))\n",
        "yval_scale=scaler_y.transform(Y_test)\n",
        "\n",
        "def denormalize(df,norm_data):\n",
        "    df = df['traffic_volume'].values.reshape(-1,1)\n",
        "    norm_data = norm_data.reshape(-1,1)\n",
        "    scl = MinMaxScaler()\n",
        "    a = scl.fit_transform(df)\n",
        "    new = scl.inverse_transform(norm_data)\n",
        "    return new\n",
        "\n",
        "\"\"\"\n",
        "Above written function for denormalizatio of data after normalizing\n",
        "this function will give original scale of values.\n",
        "In normalization we step down the value of data in dataset.\n",
        "\"\"\"\n",
        "\n",
        "def neural_net_model(X_data,input_dim):\n",
        "    W_1 = tf.Variable(tf.random_uniform([input_dim,10]))\n",
        "    b_1 = tf.Variable(tf.zeros([10]))\n",
        "    layer_1 = tf.add(tf.matmul(X_data,W_1), b_1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "\n",
        "    # layer 1 multiplying and adding bias then activation function\n",
        "    W_2 = tf.Variable(tf.random_uniform([10,10]))\n",
        "    b_2 = tf.Variable(tf.zeros([10]))\n",
        "    layer_2 = tf.add(tf.matmul(layer_1,W_2), b_2)\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # layer 2 multiplying and adding bias then activation function\n",
        "    W_O = tf.Variable(tf.random_uniform([10,1]))\n",
        "    b_O = tf.Variable(tf.zeros([1]))\n",
        "    output = tf.add(tf.matmul(layer_2,W_O), b_O)\n",
        "    # O/p layer multiplying and adding bias then activation function\n",
        "    # notice output layer has one node only since performing #regression\n",
        "    return output\n",
        "\"\"\"\n",
        "neural_net_model is function applying 2 hidden layer feed forward neural net.\n",
        "Weights and biases are abberviated as W_1,W_2 and b_1, b_2 \n",
        "These are variables with will be updated during training.\n",
        "\"\"\"\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "xs = tf.placeholder(\"float\")\n",
        "ys = tf.placeholder(\"float\")\n",
        "output = neural_net_model(xs,10)\n",
        "cost = tf.reduce_mean(tf.square(output-ys))\n",
        "# our mean squared error cost function\n",
        "\n",
        "train = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
        "# Gradinent Descent optimiztion just discussed above for updating weights and biases\n",
        "\n",
        "c_t = []\n",
        "c_test = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initiate session and initialize all vaiables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    for i in range(30):\n",
        "        for j in range(X_train.shape[0]):\n",
        "            sess.run([cost,train],feed_dict= {xs:X_train.iloc[j,:].values.reshape(1,10), ys:Y_train[j]})\n",
        "            # Run cost and train with each sample\n",
        "        c_t.append(sess.run(cost, feed_dict={xs:X_train,ys:Y_train}))\n",
        "        c_test.append(sess.run(cost, feed_dict={xs:X_test,ys:Y_test}))\n",
        "        print('Epoch :',i,'Cost :',c_t[i])\n",
        "    pred = sess.run(output, feed_dict={xs:X_test})\n",
        "    # predict output of test data after training\n",
        "    print('Cost :',sess.run(cost, feed_dict={xs:X_test,ys:Y_test}))\n",
        "    Y_test = denormalize(df_test,Y_test)\n",
        "    pred = denormalize(df_test,pred)\n",
        "    #Denormalize data     \n",
        "    plt.plot(range(Y_test.shape[0]),Y_test,label=\"Original Data\")\n",
        "    plt.plot(range(Y_test.shape[0]),pred,label=\"Predicted Data\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.ylabel('Traffic Volume Value')\n",
        "    plt.xlabel('Days')\n",
        "    plt.title('Prediction vs Actual')\n",
        "    plt.show()\n",
        "    # if input('Save model ? [Y/N]') == 'Y':\n",
        "    #     saver.save(sess,'yahoo_dataset.ckpt')\n",
        "    #     print('Model Saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9R3vS2wkG90"
      },
      "source": [
        "#PLOTTING\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter (Y_test, pred) \n",
        "range = [Y_test.min (), pred.max ()] \n",
        "plt.plot (range, range, 'red') \n",
        "plt.title('Predicted vs Actual ')\n",
        "plt.xlabel ('Actual') \n",
        "plt.ylabel ('Predicted ') \n",
        "plt.show ()\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(Y_test,label=\"Original Data\")\n",
        "plt.plot(pred,label=\"Predicted Data\")\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('Traffic Volume Value')\n",
        "plt.xlabel('Days')\n",
        "plt.title('Prediction vs Actual')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}